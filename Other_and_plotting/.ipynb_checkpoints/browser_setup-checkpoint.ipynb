{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "orf_to_gene = {i[0]:i[1] for i in np.array(pd.read_csv('../accessory_files/yeast_gene_annotations.tsv', delimiter='\\t')[['ORF', 'Gene_ORF']])}\n",
    "orf_hits = pd.read_csv('../../Output/WGS/combined_option/gene_hit_data.tsv', delimiter='\\t')\n",
    "wells = [i.split('_')[0] for i in orf_hits if '_present' in i]\n",
    "well_orfs_present = defaultdict(list)\n",
    "for j, row in orf_hits.iterrows():\n",
    "    for well in wells:\n",
    "        if row[well+'_present']>0:\n",
    "            well_orfs_present[well].append(row['Gene_ORF'])\n",
    "# One excluded autodiploid well is not in this data, so I'll add it manually:\n",
    "td = pd.read_csv('../../Output/WGS/combined_option/processed_well_output/P1B03_processed.tsv', delimiter='\\t')\n",
    "td['ORF_hit']\n",
    "well_orfs_present['P1B03'] = [orf_to_gene.get(i, i) for i in set(td['ORF_hit']) if pd.notnull(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "plates = ['P1', 'P2', 'P3']\n",
    "fds = []\n",
    "for p in plates:\n",
    "    td = pd.read_csv('../../Output/Fitness/' + p + '_freq_and_s_data.csv')\n",
    "    td['plate'] = [p]*len(td)\n",
    "    fds.append(td[['plate', 'Well'] + [i for i in td if '_s_zeroed' in i and 'R' not in i]])\n",
    "\n",
    "fit_data = pd.concat(fds)\n",
    "fit_data.columns\n",
    "fit_data['platewell'] = fit_data['plate'] + fit_data['Well']\n",
    "td = pd.read_csv('../accessory_files/VLTE_by_well_info.csv')[['platewell', 'contam', 'strain']]\n",
    "fit_data = fit_data.merge(td, on='platewell', how='left')\n",
    "pg = pd.DataFrame([[w, ';'.join(well_orfs_present[w])] for w in well_orfs_present], columns=['platewell', 'genes_w_nonsyn_muts'])\n",
    "fit_data = fit_data.merge(pg, on='platewell', how='left')  \n",
    "fit_data['focal'] = pd.notnull(fit_data['genes_w_nonsyn_muts'])\n",
    "fit_data[fit_data['focal']]\n",
    "fit_data[fit_data['plate']==p].to_csv('../../Output/Browser/well_fitness_info_etc.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wells = list(pg['platewell'])\n",
    "base_use_cols = ['CHROM', 'POS', 'REF', 'ALT', 'QUAL', 'SVTYPE', 'ANN_simpler', 'ANN', \n",
    "                 'af_trajectory', 'perc_of_alt', 'mutation_group', 'ORF_hit'\n",
    "                 'G70_allele_counts', 'G1410_allele_counts', 'G2640_allele_counts',\n",
    "                 'G5150_allele_counts', 'G7530_allele_counts', 'G10150_allele_counts']\n",
    "seq_gens = [70, 1410, 2640, 5150, 7530, 10150]\n",
    "for well in sorted(wells):\n",
    "    td = pd.read_csv('../../Output/WGS/combined_option/processed_well_output/' + well + '_processed.tsv', delimiter='\\t')\n",
    "    for gen in seq_gens:\n",
    "        if 'G'+str(gen)+'_alt_counts' in td:\n",
    "            td['G'+str(gen)+'_allele_counts'] = td.apply(lambda r: str(r['G'+str(gen)+'_ref_counts'])+','+str(r['G'+str(gen)+'_alt_counts']), axis=1)\n",
    "    svc = len(td[pd.notnull(td['SVTYPE'])])\n",
    "    #print(well, svc)\n",
    "    use_cols = [c for c in base_use_cols if c in td]\n",
    "    td[use_cols].to_csv('../../Output/Browser/Allele_freqs/' + well + '.tsv', sep='\\t', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "milo_py37",
   "language": "python",
   "name": "milo_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
